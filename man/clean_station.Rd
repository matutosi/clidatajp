% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/download_data.R
\name{clean_station}
\alias{clean_station}
\title{Donwload links for areas, countries and stations.}
\usage{
clean_station(text)
}
\arguments{
\item{url}{A String to specify target html.}
}
\value{
\if{html}{\out{<div class="sourceCode">}}\preformatted{A string vector of url links.
}\if{html}{\out{</div>}}
}
\description{
For polite scraping, 5 sec interval is set in donwload_links(),
it takes about 15 minutes to get all station links.
Please use existing links by "data(station_links)",
if you do not need to renew links.
You can see web page as below.
https://www.data.jma.go.jp/gmd/cpd/monitor/nrmlist/
}
\examples{
# If you want links for all countries and all sations, remove head().
library(tidyverse)
area_links <- donwload_area_links()
station_links <- NULL
area_links <- head(area_links)  # for test
for(i in seq_along(area_links)){
    print(stringr::str_c(i, " / ", length(area_links)))
    country_links <- donwload_links(area_links[i])
    country_links <- head(country_links)  # for test
    for(j in seq_along(country_links)){
        print(stringr::str_c("    ", j, " / ", length(country_links)))
        station_links <- c(station_links, donwload_links(country_links[j]))
    }
}
tibble::tibble(url = station_links) \%>\%
  head()

}
